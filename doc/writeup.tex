\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, moreverb, url, verbatim, url}

\begin{document} 

\title{{11-761 Language and Statistics\\Spring 2012\\Course Project}}
\author{Ryan Carlson, Naoki Orii, Peter Schulam}
\maketitle

\section{Description of the Toolkit}

We linearly interpolated n-gram models for $1 \le n \le 8$ with a maxent model. The n-gram models were implemented by hand in python. For the maxent model we used the ``Maximum Entropy Modeling Toolkit for Python and C++''\footnote{\url{http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html}} (we used the python bindings). Finally, we used the interpolation code from a previous assignment to linearly interpolate the nine models together.

Our internal evaluation is as follows: 37.59\% accuracy and 8.101 bit/tag\footnote{89.8\%  9.98\%, and 0.2\% of trainA.txt were used as training, dev, and test data, respectively}.

\section{Contributions}

<<<<<<< HEAD
We all wrote the n-gram models together, Peter implemented add-one smoothing to the n-grams (and the unsuccessful Good-Turing), the testing code, and added the maxent features detailed in sections \ref{sec:constituent}. Naoki set up the maxent model and added maxent features detailed in sections \ref{sec:trigger} and \ref{sec:longdistance}. Ryan worked on the initial n-gram class, the training code, and maxent features detailed in sections \textbf{ADD SECTION REFS}.
=======
We all wrote the n-gram models together, Peter implemented add-one smoothing to the n-grams, the testing code, and added the maxent features detailed in sections \ref{sec:constituent}. Naoki set up the maxent model and added maxent features detailed in sections \ref{sec:trigger} and \ref{sec:longdistance}. Ryan worked on the initial n-gram class, the training code, and maxent features detailed in section \ref{sec:subset-existence}. 
>>>>>>> 31d23940129e799916e7a46da48e09ef43ce35ad

\subsection{N-gram Models}

To capture short-range dependencies between part of speech tags, we use
eight different N-gram models (unigram, bigram, trigram, etc.). The
parameters are estimated using maximum likelihood with pseudo counts to
avoid assigning N-grams zero probabilities. In addition to the standard
N-gram models, we also added unigram and bigram features to our maximum
entropy model. 

\subsection{Triggers}
\label{sec:trigger}
N-grams can not capture long distance information.
For example, if we have observed a left parenthesis in a given sentence,
there is a highly likelihood that we will observe a right parenthesis in the same sentence,
but n-grams will fail to predict this.
We capture this long distance information by adding triggers pairs as feature functions.
To formulate a trigger pair $A \rightarrow B$ as a constraint, we define the feature function $f_{A \rightarrow B}$ as:
\[
  f_{A \rightarrow B}(h, w) = \begin{cases}
    1 & (\textrm{if } A \in h \textrm{ and } w = B) \\
    0 & (otherwise)
  \end{cases}
\]
where $h$ and $w$ denote the history and the word, respectively.

Using the training data, we computed the average mutual information for the 1089 possible triggers pairs.
In Table \ref{t:triggerpairs}, we list trigger pairs and their corresponding mutual information (MI) values, sorted by decreasing order of MI.

\begin{table*}[h!]
\begin{small}
\begin{center}
\caption{Trigger {\sf A} for word {\sf B}, sorted by MI in decreasing order}
\label{t:triggerpairs}
\begin{tabular}{|l|l|l|}
\hline
{\sf A} & {\sf B} & {\sf Mutual Information} \\
\hline
CD & CD & 0.00933 \\
$<$LEFTPAR$>$ & $<$RIGHTPAR$>$ & 0.00443 \\
$<$PERIOD$>$ & $<$PERIOD$>$ & 0.00431 \\
VBD & VBD & 0.00307 \\
NNP & NNP & 0.00302 \\
VBZ & CD & 0.00279 \\
PRP & CD & 0.00259 \\
$<$COLON$>$ & $<$COLON$>$ & 0.00248 \\
VB & CD & 0.00233 \\
VBZ & VBD & 0.00226 \\
VBP & CD & 0.00196 \\
VBD & VBZ & 0.00169 \\
PRP & PRP & 0.00151 \\
VBZ & VBZ & 0.00145 \\
VBD & VBP & 0.00144 \\
VBP & VBP & 0.00141 \\
VBP & VBD & 0.00140 \\
VBD & CD & 0.00131 \\
RB & CD & 0.00123 \\
DT & CD & 0.00113 \\
MD & CD & 0.000944 \\
... & ... & ... \\
\hline
\end{tabular}\vspace*{-5mm}
\end{center}
\end{small}
\end{table*}

It can be seen from the table that {\em self-triggers}, or words that trigger themselves (such as CD $\rightarrow$ CD) comprise the majority of the top 10 trigger pairs.
As expected, we see that $<$LEFTPAR$>$ $\rightarrow$ $<$RIGHTPAR$>$ has a high mutual information.
Similar to Rosenfeld \cite{rosenfeld1996}, we only incorporated pairs that had at least 0.001 bit of average mutual information into our system.


\subsection{Long Distance N-grams}
\label{sec:longdistance}

Long distance $N$-grams are extensions of $N$-grams where a word is predicted from $N-1$-grams some distance back in the history.
For example, a distance-2 bigram predicts $w_i$ from the history $w_{i-2}$.
Constraints for distance-$j$ bigram $\{w_1, w_2\}$ can be formulated as follows:
\[
  f_{\{w_1, w_2\}}^{j}(h, w) = \begin{cases}
    1 & (\textrm{if } w_{i-j} = w_1 \textrm{ and } w = w_2) \\
    0 & (otherwise)
  \end{cases}
\]

\noindent
In our system, we considered distance-2 bigrams and distance-2 trigrams.

\subsection{Shallow Parse Tree Constituents}
\label{sec:constituent}

To introduce aspects of syntax into our toolkit, we defined a number of
binary valued ``chunk rule'' features for our maximum entropy model. The
general idea is to use a number of production rules taken from a
context-free grammar to check for higher level syntactic structure. For
example, suppose that the current history of characters in the token
stream is \texttt{DT NN VB NN PP DT}, and we want to know the
probability that the next token is \texttt{NN}. If we have the 
production rule below defined in our shallow grammar file, then the
feature \texttt{feature\_PP} will be true when calculating the
probability \texttt{P(NN|DT NN VB NN PP DT)}.

\begin{verbatim}
PP -> PP DT NN
\end{verbatim}

We gathered chunk rules by first going through a context free grammar
for English that we used in another class, and cleaning the part of
speech tag set used in the grammar to match the ones in our tag set.
Since there were some tags that did not have any analogues in our set,
we then kept only those production rules for which the right hand side
of the rule contained only tags that were in our tag set. This removed
most of the ``high level'' tags that can only be built higher in a parse
tree, which is why we call these features ``shallow'' features (these
rules, and the grammar can be found in the \texttt{resources}
directory).

\subsection{Checking For Subset Existence}
\label{sec:subset-existence}

In order to get a natural sense of higher-level parts of speech (e.g., nouns, verbs, punctuation) we grouped the tags into the following lists:

\begin{verbatim}
[JJ, JJR, JJS] # adjectives
[NN, NNS, NNP, NNPS] # nouns
[PRP, PRP$] # pronouns
[RB, RBR] # adverbs
[VB, VBD, VBG, VBN, VBP, VBZ] # verbs
[WDT, WP, WRB] # wh words
[<COLON>, <COMMA>, <LEFTPAR>, <PERIOD>, <RIGHTPAR>] # punctuation
[CC] # coordinating conjunction
[CD] # cardinal number
[DT] # determiner
[EX] # existential THERE
[IN] # preposition or subordinating conjunction
[MD] # modal
[POS] # possessive ending
[RP] # particle
[TO] # TO
[CC, CD, DT, EX, IN, MD, POS, RP, TO] #other
\end{verbatim}

Each of the seven major parts of speech groups are represented, and then each
of the rest are in both a singleton group and in a special ``other'' group. We
do this because the nine singletons do not really group together in any
linguistically meaningful way, so we let the maxent model decide whether or
not an ``other'' category has more predictive power than each of the features
on their own. We could have taken this a step further and allowed any
combination of the features to group together, but this would have both become
too computationally expensive and would not have had the linguistic power we
wanted.

Now, for each group $G$, a lookback number $i$, and a history \[ H = \{h_1,
h_2, \dots, h_n\},\] we check if

\[ \forall g \in G,\; g \in \{h_{n}, h_{n-1}, \ldots, h_{n-i+1}\}  \]

For example, suppose $G$ was the group of adjectives (i.e., $g$ could take the
value \texttt{JJ}, \texttt{JR}, or \texttt{JJS}) and that the lookback number
$i = 2$. Furthermore, suppose the history was \texttt{\{<PERIOD>, DT, JJ,
NN\}}. Then the adjective \texttt{JJ} would fire (evaluate to \texttt{True})
because it matches the second to last word in the history. However, if the
target group $G$ were determiners, the feature would not fire because the
determiner \texttt{DT} is too far in the past.

\section{Comments and Suggestions}

The only thing we would suggest is to give future classes a little more time.
We're not sure exactly how you'd manage that, given that you told us right
after the final, but if you can figure out a way to make it work it'd be nice
to have more time.

\begin{thebibliography}{}

\bibitem{rosenfeld1996} 
R. Rosenfeld,
``A Maximum Entropy Approach to Adaptive Statistical Language Modeling,''
{\em Computer, Speech, and Language}, vol. 10, pp.187-228, 1996.

\end{thebibliography}

\end{document}
